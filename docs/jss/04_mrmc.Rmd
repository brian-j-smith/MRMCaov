# Multi-Reader Multi-Case Analysis

A multi-reader multi-case (MRMC) analysis, as the name suggests, involves multiple readers of multiple cases to compare reader performance metrics across two or more diagnostic tests.  An MRMC analysis can be performed with a call to the `mrmc()` function to specify a reader performance metric, study variables and observations, and covariance estimation method.

\begin{tcolorbox}[title=MRMC Function]
\textbf{Syntax}
\begin{verbatim}
mrmc(response, test, reader, case, data, cov = jackknife)
\end{verbatim}
\begin{Description}
Returns an \code{mrmc} class object of data that can be used to estimate and compare reader performance metrics in a multi-reader multi-case statistical analysis.
\end{Description}
\textbf{Arguments}
\begin{description}
\item[\code{response}:] object defining true case statuses, corresponding reader ratings, and a reader performance metric to compute on them.
\item[\code{test}, \code{reader}, \code{case}:] variables containing the test, reader, and case identifiers for the \code{response} observations.
\item[\code{data}:] data frame containing the response and identifier variables.
\item[\code{cov}:] function \code{jackknife}, \code{unbiased}, or \code{DeLong} to estimate reader performance metric covariances.
\end{description}
\end{tcolorbox}

The response variable in the `mrmc()` specification is defined with one of the performance metrics described in the following sections.  Results from `mrmc()` can be displayed with `print()` and passed to `summary()` for statistical comparisons of the diagnostic tests.  The summary call produces ANOVA results from a global test of equality of ROC AUC means across all tests and statistical tests of pairwise differences, along with confidence intervals for the differences and intervals for individual tests.

\begin{tcolorbox}[title=MRMC Summary Function]
\textbf{Syntax}
\begin{verbatim}
summary(object, conf.level = 0.95)
\end{verbatim}
\begin{Description}
Returns a \code{summary.mrmc} class object of statistical results from a multi-reader multi-case analysis.
\end{Description}
\textbf{Arguments}
\begin{description}
\item[\code{object}:] results from \code{mrmc()}.
\item[\code{conf.level}:] confidence level for confidence intervals.
\end{description}
\end{tcolorbox}


## Performance Metrics


### Area Under the ROC Curve

Area under the ROC curve is a measure of concordance between numeric reader ratings and true binary case statuses.  It provides an estimate of the probability that a randomly selected positive case will have a higher rating than a negative case.  ROC AUC values range from 0 to 1, with 0.5 representing no concordance and 1 perfect concordance.  AUC can be computed with the functions described below for binormal, binormal likelihood-ratio, and empirical ROC curves.  Empirical curves are also referred to as trapezoidal.  The functions also support calculation of partial AUC over a range of sensitivities or specificities.

\begin{tcolorbox}[title=ROC AUC Functions]
\textbf{Syntax}
\begin{footnotesize}
\begin{verbatim}
binormal_auc(truth, rating, partial = FALSE, min = 0, max = 1, normalize = FALSE) 
binormalLR_auc(truth, rating, partial = FALSE, min = 0, max = 1, normalize = FALSE)
empirical_auc(truth, rating, partial = FALSE, min = 0, max = 1, normalize = FALSE) 
trapezoidal_auc(truth, rating, partial = FALSE, min = 0, max = 1, normalize = FALSE)
\end{verbatim}
\end{footnotesize}
\begin{Description}
Returns computed area under the receiver operating character curve estimated with a binormal model (\code{binormal\_auc}), binormal likelihood-ratio model (\code{binormalLR\_auc}), or empirically (\code{empirical\_auc} or \code{trapezoidal\_auc}).
\end{Description}
\textbf{Arguments}
\begin{description}
\item[\code{truth}:] vector of true binary case statuses, with positive status taken to be the highest level.
\item[\code{rating}:] numeric vector of case ratings.
\item[\code{partial}:] character string \code{"sensitivity"} or \code{"specificity"} for calculation of partial AUC, or \code{FALSE} for full AUC.  Partial matching of the character strings is allowed.  A value of \code{"specificity"} results in area under the ROC curve between the given \code{min} and \code{max} specificity values, whereas \code{"sensitivity"} results in area to the right of the curve between the given sensitivity values.
\item[\code{min}, \code{max}:] minimum and maximum sensitivity or specificity values over which to calculate partial AUC.
\item[\code{normalize}:] logical indicating whether partial AUC is divided by the interval width (\code{max - min}) over which it is calculated.
\end{description}
\end{tcolorbox}

In the example below, `mrmc()` is called to compare CINE MRI and SE MRI treatments in an MRMC analysis of areas under binormal ROC curves computed for the readers of cases in the VanDyke study.

```{r using_mrmc_roc}
## Compare ROC AUC treatment means for the VanDyke example
est <- mrmc(
  binormal_auc(truth, rating), treatment, reader, case, data = VanDyke
)
```

The `print()` function can be applied to `mrmc()` output to display information about the reader performance metrics, including the

* value of variable `truth` (1) defining positive case status,
* estimated performance metric values (`data$binormal_auc`) for each test (`$treatment`) and reader (`$reader`),
* number of cases read at each level of the factors (`N`), and
* error variance $\sigma^2_\epsilon$ and covariances $\text{Cov}_1$, $\text{Cov}_2$, and $\text{Cov}_3$.

**MRMC Performance Metrics**

```{r}
print(est)
```

MRMC statistical tests are performed with a call to `summary()`.  Results include a test of the global null hypothesis that performances are equal across all diagnostic tests, tests of their pairwise mean differences, and estimated mean performances for each one.

**MRMC Test Results**

```{r}
summary(est)
```

ROC curves estimated by `mrmc()` can be displayed with `plot()` and their parameters extracted with `parameters()`.

**MRMC ROC Curves**

```{r}
plot(est)
```

**MRMC ROC Curve Parameters**

```{r}
print(parameters(est))
```


### ROC Curve Expected Utility

As an alternative to AUC as a summary of ROC curves, Abbey et al. [-Aabbey:2013:SPC] propose an expected utility metric defined as
$$
\text{EU} = \max_\text{FPR}(\text{TPR}(\text{FPR}) - \beta \times \text{FPR}),
$$
where $\text{TPR}(\text{FPR})$ are true positive rates on the ROC curve, and FPR are false positive rates ranging from 0 to 1.  From a decision theory perspective, expected utility can be viewed as the expected loss of classifying cases and is minimized when $\beta = (1 - p) / (r \times p)$, where $p$ is the population prevalence of positive cases and $r$ is the cost associated with a false negative classification relative to a false positive one [@Perkins:2006:IOC].  Accordingly, expected utility could be used to compare diagnostic tests with respect to the ``optimalities'' of their classifications for a specified disease prevalence $p$ and relative cost of incorrect classifications $r$.

\begin{tcolorbox}[title=ROC Curve Expected Utility Functions]
\textbf{Syntax}
\begin{verbatim}
binormal_eu(truth, rating, slope = 1) 
binormalLR_eu(truth, rating, slope = 1)
empirical_eu(truth, rating, slope = 1) 
trapezoidal_eu(truth, rating, slope = 1)
\end{verbatim}
\begin{Description}
Returns expected utility of an ROC curve.
\end{Description}
\textbf{Arguments}
\begin{description}
\item[truth:] vector of true binary case statuses, with positive status taken to be the highest level.
\item[\code{rating}:] numeric vector of case ratings.
\item[\code{slope}:] numeric slope ($\beta$) at which to compute expected utility.
\end{description}
\end{tcolorbox}


### ROC Curve Sensitivity and Specificity

Functions are provided to extract sensitivity from an ROC curve for a given specificity and vice versa.

\begin{tcolorbox}[title=ROC Curve Sensitivity and Specificity Functions]
\textbf{Syntax}
\begin{verbatim}
binormal_sens(truth, rating, spec)
binormal_spec(truth, rating, sens)
binormalLR_sens(truth, rating, spec)
binormalLR_spec(truth, rating, sens)
empirical_sens(truth, rating, spec)
empirical_spec(truth, rating, sens)
trapezoidal_sens(truth, rating, spec)
trapezoidal_spec(truth, rating, sens)
\end{verbatim}
\begin{Description}
Returns the sensitivity/specificity from an ROC curve at a specified specificity/sensitivity.
\end{Description}
\textbf{Arguments}
\begin{description}
\item[\code{truth}:] vector of true binary case statuses, with positive status taken to be the highest level.
\item[\code{rating}:] numeric vector of case ratings.
\item[\code{spec}, \code{sens}:] specificity/sensitivity on the ROC curve at which to return sensitivity/specificity.
\end{description}
\end{tcolorbox}


### Binary Metrics

Metrics for binary reader ratings are also available.

\begin{tcolorbox}[title=Sensitivity and Specificity Functions]
\textbf{Syntax}
\begin{verbatim}
binary_sens(truth, rating)
binary_spec(truth, rating)
\end{verbatim}
\begin{Description}
Returns the sensitivity or specificity.
\end{Description}
\textbf{Arguments}
\begin{description}
\item[\code{truth}:] vector of true binary case statuses, with positive status taken to be the highest level.
\item[\code{rating}:] factor or numeric vector of 0-1 binary ratings.
\end{description}
\end{tcolorbox}

```{r using_mrmc_binary}
## Compare sensitivity for binary classification
VanDyke$binary_rating <- VanDyke$rating >= 3
est <- mrmc(
  binary_sens(truth, binary_rating), treatment, reader, case, data = VanDyke
)
```

**MRMC Performance Metrics**

```{r}
print(est)
```

**MRMC Test Results**

```{r}
summary(est)
```


## Covariance Estimation Methods

Special statistical methods are needed in MRMC analyses to estimate covariances between performance metrics from different readers and tests when cases are treated as a random sample and are rated by more than one reader or evaluated with more than one test.  For this estimation, the package provides the DeLong method [@DeLong:1988:CAU], jackknifing [@Efron:1982:JBR], and an unbiased method [@Gallas:2007:MMV].  The applicability of each depends on the study design as well as the performance metric being analyzed, as summarized in Table~\ref{tbl:cov}.  DeLong is appropriate for a balanced factorial design and empirical ROC AUC, jackknifing for any design and metric, and unbiased for any design and empirical ROC AUC.

\begin{table}
\caption{MRMC covariance estimation methods and functions available per study design and reader performance metric}
\label{tbl:cov}
\begin{tabular}{llll}
\toprule
Covariance Method    & Study Design & Metric            & Function      \\
\midrule
DeLong               & Factorial    & Empirical ROC AUC & \code{DeLong()}    \\
Jackknife            & Any          & Any               & \code{jackknife()} \\
Unbiased             & Any          & Empirical ROC AUC & \code{unbiased()}  \\
\bottomrule
\end{tabular}
\end{table}

Jackknifing is the default covariance method for `mrmc()`.  Others can be specified with its `cov` argument.

```{r using_mrmc_DeLong}
## DeLong method
est <- mrmc(
  empirical_auc(truth, rating), treatment, reader, case, data = VanDyke,
  cov = DeLong
)
```

**MRMC Test Results**

```{r}
summary(est)
```

```{r using_mrmc_unbiased}
## Unbiased method
est <- mrmc(
  empirical_auc(truth, rating), treatment, reader, case, data = VanDyke,
  cov = unbiased
)
```

**MRMC Test Results**

```{r}
summary(est)
```


## Fixed Factors

By default, readers and cases are treated as random effects by `mrmc()`.  Random effects are the appropriate designations when inference is intended for the larger population from which study readers and cases are considered to be a random sample.  Either, but not both, can be specified as fixed effects with the `fixed()` function in applications where study readers or cases make up the entire group to which inference is intended.  When readers are designated as fixed, `mrmc()` test results additionally include reader-specific pairwise comparisons of the diagnostic tests as well as mean estimates of the performance metric for each reader-test combination.

```{r using_mrmc_fixed_readers}
## Fixed readers
est <- mrmc(
  empirical_auc(truth, rating), treatment, fixed(reader), case, data = VanDyke
)
```

**MRMC Test Results**

```{r}
summary(est)
```

```{r using_mrmc_fixed_cases}
## Fixed cases
est <- mrmc(
  empirical_auc(truth, rating), treatment, reader, fixed(case), data = VanDyke
)
```

**MRMC Test Results**

```{r}
summary(est)
```


## Study Designs

**MRMCaov** supports factorial, nested, and partially paired study designs.  In a factorial design, one set of cases is evaluated by all readers and tests.  This is the design employed by the VanDyke study as indicated by its dataset `case` identifier values which appear within each combination of the `reader` and `treatment` identifiers.  Designs in which a different set of cases is evaluated by each reader or with each test can be specified with unique codings of case identifiers within the corresponding nesting factor.  Example codings for these two nested designs are included in the `VanDyke` dataset as `case2` and `case3`.  The `case2` identifiers differ from reader to reader and thus represent a study design in which cases are nested within readers.  Likewise, the `case3` identifiers differ by test and are an example design of cases nested within tests.  Additionally, the package supports partially paired designs in which ratings may not be available on all cases for some readers or tests; e.g., as a result of missing values.  Nested and partially paired designs require specification of jackknife (default) or unbiased as the covariance estimation method.

```{r echo=FALSE}
cat("Case identifier codings for factorial and nested study designs\n")
x <- t(VanDyke[c("reader", "treatment", "case", "case2", "case3")])
dimnames(x) <- list(
  Factor = rownames(x),
  Observation = seq_len(ncol(x))
)
n <- 30
print(x[, seq_len(n)], quote = FALSE)
cat("... with", ncol(x) - n, "more observations")
```

```{r using_mrmc_within_readers}
## Cases nested within readers
est <- mrmc(
  empirical_auc(truth, rating), treatment, reader, case2, data = VanDyke
)
```

**MRMC Test Results**

```{r}
summary(est)
```

```{r using_mrmc_within_tests}
## Cases nested within tests
est <- mrmc(
  empirical_auc(truth, rating), treatment, reader, case3, data = VanDyke
)
```

**MRMC Test Results**

```{r}
summary(est)
```
